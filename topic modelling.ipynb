{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "comments = pd.read_csv('comments.csv',delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.drop(comments.columns[1],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gsevr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['possible', 'social', 'media', 'already', 'cost', 'us', 'american', 'democracy', 'right', 'confident', 'beliefs', 'generally', 'accurate', 'least', 'respect', 'things', 'put', 'effort', 'sure', 'able', 'continue', 'saying', 'llm', 'generated', 'materials', 'text', 'images', 'videos', 'etc', 'became']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "data = comments.comment.values.tolist()\n",
    "data_words = list(sent_to_words(data))# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.043*\"ai\" + 0.007*\"human\" + 0.006*\"like\" + 0.006*\"would\" + 0.006*\"people\" '\n",
      "  '+ 0.004*\"us\" + 0.004*\"even\" + 0.004*\"companies\" + 0.004*\"tech\" + '\n",
      "  '0.004*\"think\"'),\n",
      " (1,\n",
      "  '0.018*\"ai\" + 0.009*\"technology\" + 0.008*\"us\" + 0.007*\"people\" + '\n",
      "  '0.007*\"human\" + 0.005*\"would\" + 0.005*\"could\" + 0.005*\"need\" + '\n",
      "  '0.004*\"things\" + 0.004*\"like\"'),\n",
      " (2,\n",
      "  '0.014*\"ai\" + 0.008*\"us\" + 0.006*\"like\" + 0.005*\"going\" + 0.005*\"systems\" + '\n",
      "  '0.005*\"could\" + 0.005*\"would\" + 0.005*\"want\" + 0.004*\"well\" + '\n",
      "  '0.004*\"technology\"'),\n",
      " (3,\n",
      "  '0.022*\"ai\" + 0.006*\"like\" + 0.006*\"could\" + 0.006*\"human\" + 0.005*\"humans\" '\n",
      "  '+ 0.005*\"people\" + 0.005*\"one\" + 0.005*\"technology\" + 0.005*\"us\" + '\n",
      "  '0.005*\"think\"'),\n",
      " (4,\n",
      "  '0.012*\"ai\" + 0.011*\"us\" + 0.008*\"people\" + 0.005*\"human\" + 0.005*\"one\" + '\n",
      "  '0.004*\"humans\" + 0.004*\"see\" + 0.004*\"technology\" + 0.003*\"need\" + '\n",
      "  '0.003*\"stop\"'),\n",
      " (5,\n",
      "  '0.009*\"ai\" + 0.007*\"would\" + 0.006*\"humans\" + 0.005*\"people\" + '\n",
      "  '0.005*\"think\" + 0.004*\"technology\" + 0.004*\"us\" + 0.004*\"make\" + '\n",
      "  '0.004*\"human\" + 0.004*\"could\"'),\n",
      " (6,\n",
      "  '0.009*\"ai\" + 0.007*\"would\" + 0.007*\"people\" + 0.007*\"think\" + 0.006*\"like\" '\n",
      "  '+ 0.005*\"make\" + 0.005*\"could\" + 0.004*\"us\" + 0.004*\"get\" + '\n",
      "  '0.004*\"technology\"'),\n",
      " (7,\n",
      "  '0.019*\"ai\" + 0.008*\"people\" + 0.005*\"already\" + 0.005*\"human\" + '\n",
      "  '0.005*\"world\" + 0.005*\"think\" + 0.003*\"could\" + 0.003*\"technology\" + '\n",
      "  '0.003*\"never\" + 0.003*\"us\"'),\n",
      " (8,\n",
      "  '0.018*\"ai\" + 0.012*\"us\" + 0.007*\"people\" + 0.006*\"human\" + 0.006*\"like\" + '\n",
      "  '0.005*\"one\" + 0.005*\"think\" + 0.005*\"make\" + 0.005*\"humans\" + '\n",
      "  '0.004*\"could\"'),\n",
      " (9,\n",
      "  '0.020*\"ai\" + 0.007*\"could\" + 0.007*\"like\" + 0.007*\"us\" + 0.007*\"even\" + '\n",
      "  '0.006*\"would\" + 0.005*\"think\" + 0.005*\"humanity\" + 0.005*\"human\" + '\n",
      "  '0.004*\"control\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "# # Visualize the topics\n",
    "# pyLDAvis.enable_notebook()\n",
    "\n",
    "# LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
    "\n",
    "# # # this is a bit time consuming - make the if statement True\n",
    "# # # if you want to execute visualization prep yourself\n",
    "\n",
    "# if 1 == 1:\n",
    "#     LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "#     with open('test', 'wb') as f:\n",
    "#         pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# # load the pre-prepared pyLDAvis data from disk\n",
    "# with open('test', 'rb') as f:\n",
    "#     LDAvis_prepared = pickle.load(f)\n",
    "    \n",
    "# pyLDAvis.save_html(LDAvis_prepared, 'test'+ str(num_topics) +'.html')\n",
    "    \n",
    "# LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "pyLDAvis.save_html(vis, 'test_html.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
